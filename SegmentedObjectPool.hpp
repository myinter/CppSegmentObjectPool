/*
 * SegmentedObjectPool.hpp
 *
 * Copyright (c) 2025 å¤§ç†Šå“¥å“¥ (Bighiung)
 *
 * ä½¿ç”¨è®¸å¯ / License Terms:
 *
 * æœ¬ä»£ç å…è®¸åœ¨ä¸ªäººã€å­¦æœ¯åŠå•†ä¸šé¡¹ç›®ä¸­è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘ï¼Œ
 * ä½†å¿…é¡»åœ¨æ‰€æœ‰å‰¯æœ¬åŠè¡ç”Ÿä½œå“ä¸­ä¿ç•™æœ¬å£°æ˜ï¼Œä¸”æ˜ç¡®æ ‡æ³¨ä½œè€…ä¸ºï¼š
 *
 *      å¤§ç†Šå“¥å“¥ (Bighiung)
 *
 * ç¦æ­¢å»é™¤æˆ–ä¿®æ”¹æ­¤ç‰ˆæƒå£°æ˜ã€‚
 *
 * This code is free to use, modify, and distribute in personal,
 * academic, and commercial projects, provided that this notice
 * is retained in all copies or derivative works, and the author
 * is explicitly acknowledged as:
 *
 *      å¤§ç†Šå“¥å“¥ (Bighiung)
 *
 * Removal or alteration of this copyright notice is prohibited.
 */

/*
 * SegmentedObjectPool å¯¹è±¡æ± æ¨¡æ¿ / Segmented Object Pool Template
 *
 * åŠŸèƒ½ / Features:
 * 1. ç»Ÿä¸€åˆ†é…å¯¹è±¡ / Unified object allocation
 * 2. é¿å…å¯¹è±¡åå¤ç”³è¯·é‡Šæ”¾ / Avoid frequent allocation/deallocation
 * 3. å¯¹è±¡åœ¨å†…å­˜ä¸­è¿ç»­åœ°å€åˆ†é… / Objects allocated in contiguous memory
 * 4. å¯¹è±¡è¿ç»­åˆ†é…ï¼Œæå¤§æé«˜éå†å¯¹è±¡çš„è¿‡ç¨‹ä¸­çš„ç¼“å­˜å‘½ä¸­ç‡ / Continuous object allocation significantly reduces the occurrence of cache misses when accessing objects.
 * 5. ä½¿ç”¨æ•°ç»„ç´¢å¼•ä»£æ›¿é“¾è¡¨ç®¡ç†å¯ç”¨å¯¹è±¡ï¼Œæé«˜å†…å­˜å±€éƒ¨æ€§ / Use array indices instead of linked list to manage free objects, improving memory locality.
 * 6. ç”³è¯·ç©ºé—´å¤§å°ä¾æ®æ“ä½œç³»ç»Ÿçš„å†…å­˜é¡µé¢å¤§å°,æœ€é«˜æ•ˆåˆ©ç”¨å†…å­˜,æœç»å†…éƒ¨ç¢ç‰‡ã€‚å¹¶ä¸”ä»¥åˆ†æ®µæ–¹å¼åŠ¨æ€ç”³è¯·å†…å­˜è¿›è¡Œå¯¹è±¡æ± æ‰©å®¹ã€‚ / The size of the allocated space is determined by the memory page size of the operating system. This ensures the most efficient use of memory and eliminates internal fragmentation. And dynamically apply for memory in segments to expand the object pool.
 * 7. é€‚ç”¨äºå³æ—¶æ¶ˆæ¯ã€é«˜é¢‘äº¤æ˜“ç³»ç»Ÿã€æ¸¸æˆæ•°æ®ç­‰æ€§èƒ½æ•æ„Ÿåœºæ™¯ / Suitable for IM, high frequency trading,game data, and other performance-sensitive scenarios
 * 8. å¸¦æœ‰Atomic APIs å¯ä»¥ç”¨äºå¹¶å‘ç¯å¢ƒåˆ›å»ºå’Œå›æ”¶å¯¹è±¡ / With the Atomic API, objects can be created and reclaimed in a concurrent environment.

 */

#pragma once
#include <cstddef>
#include <cstdint>
#include <new>
#include <vector>
#include <memory>
#include <type_traits>
#include <cassert>
#include <utility>
#include <iostream>
#include <algorithm>
#include <unistd.h>

#if defined(_WIN32)
  #define NOMINMAX
  #include <windows.h>
#endif

namespace detail {
inline std::size_t os_page_size() noexcept {
#if defined(_WIN32)
    SYSTEM_INFO si{};
    GetSystemInfo(&si);
    return static_cast<std::size_t>(si.dwPageSize ? si.dwPageSize : 4096);
#elif defined(_SC_PAGESIZE)
    long pz = ::sysconf(_SC_PAGESIZE);
    return static_cast<std::size_t>(pz > 0 ? pz : 4096);
#elif defined(__APPLE__) && defined(__MACH__)
    long page_size = getpagesize();
    return page_size;
#else
    return 4096;
#endif
}

constexpr std::size_t gcd(std::size_t a, std::size_t b) noexcept {
    while (b != 0) { auto t = a % b; a = b; b = t; }
    return a;
}

constexpr std::size_t lcm(std::size_t a, std::size_t b) noexcept {
    return (a / gcd(a,b)) * b;
}

constexpr std::size_t round_up(std::size_t x, std::size_t align) noexcept {
    if (align == 0) return x;
    std::size_t r = x % align;
    return r ? (x + (align - r)) : x;
}
} // namespace detail

// ----------------------------
// SegmentedObjectPool å®šä¹‰ / Definition
// ----------------------------
template <class T>
class SegmentedObjectPool {
    static_assert(!std::is_abstract<T>::value, "T must be a complete, non-abstract type");

    struct Segment {
        std::byte* data = nullptr;                // å†…å­˜å— / Memory block
        std::size_t capacity = 0;                 // å¯å®¹çº³å¯¹è±¡æ•° / Number of objects
        std::size_t next_uninit = 0;              // å°šæœªæ„é€ çš„ä¸‹ä¸€ä¸ªç´¢å¼• / Next uninitialized index

        Segment() = default;
        Segment(std::byte* d, std::size_t cap) : data(d), capacity(cap), next_uninit(0) /*free_flags(cap, 0), free_hint(0)*/ {}
    };

    // ç”¨äºçº¿ç¨‹å®‰å…¨åœºæ™¯çš„è‡ªæ—‹é” Spin lock for thread-safe scenarios
    // ç”¨äºç¡®ä¿å¯¹è±¡å›æ”¶çš„çº¿ç¨‹å®‰å…¨ Used to ensure thread safety for object recycling
    struct SpinLock {
        std::atomic_flag flag = ATOMIC_FLAG_INIT;
        inline void lock() noexcept {
            while (flag.test_and_set(std::memory_order_acquire)) {
#if defined(__x86_64__) || defined(__i386__)
                __builtin_ia32_pause();
#endif
            }
        }
        inline void unlock() noexcept {
            flag.clear(std::memory_order_release);
        }
    };

    // RAII LockGuard for facilitating the use of spin locks
    // ç®€åŒ–è‡ªæ—‹é”ä½¿ç”¨çš„ RAII LockGuard
    struct LockGuard {
        SpinLock& lock;
        explicit LockGuard(SpinLock& l) : lock(l) { lock.lock(); }
        ~LockGuard() { lock.unlock(); }
    };

    std::stack<T*> free_stack_;   // ç©ºé—²å¯¹è±¡æ ˆ Free objects stack

public:
    using value_type = T;

    inline static SegmentedObjectPool& instance() {
        static SegmentedObjectPool inst;
        return inst;
    }

    explicit SegmentedObjectPool(std::size_t min_pages_per_segment = 0, double growth = 1.0)
    : page_size_(detail::os_page_size()),
      slot_size_(detail::round_up(std::max(sizeof(T), sizeof(void*)), alignof(T))),
      pages_per_segment_base_(compute_min_pages(min_pages_per_segment)),
      growth_factor_(growth > 1.0 ? growth : 1.0) {}

    ~SegmentedObjectPool() { clear(); }
    SegmentedObjectPool(const SegmentedObjectPool&) = delete;
    SegmentedObjectPool& operator=(const SegmentedObjectPool&) = delete;

    // åˆ†é…å¯¹è±¡ / Allocate object
    template <class... Args>
    T* allocate(Args&&... args) {
        // 1. ä¼˜å…ˆä½¿ç”¨ stack ä¸­çš„ç©ºé—²å¯¹è±¡
        if (!free_stack_.empty()) {
            T* obj = free_stack_.top();
            free_stack_.pop();
            new (obj) T(std::forward<Args>(args)...);
            obj->mark_in_use();
            ++live_count_;
            return obj;
        }

        // 2. åˆ†é…æœªåˆå§‹åŒ–ç©ºé—´
        if (!segments_.empty()) {
            Segment& seg = segments_.back();
            if (seg.next_uninit < seg.capacity) {
                std::byte* ptr = seg.data + seg.next_uninit * slot_size_;
                ++seg.next_uninit;
                T* obj = reinterpret_cast<T*>(ptr);
                new (obj) T(std::forward<Args>(args)...);
                obj->mark_in_use();
                ++live_count_;
                return obj;
            }
        }

        // 3. æ‰©å®¹æ–°æ®µ
        add_segment_();
        Segment& seg = segments_.back();
        std::byte* ptr = seg.data + seg.next_uninit * slot_size_;
        ++seg.next_uninit;
        T* obj = reinterpret_cast<T*>(ptr);
        new (obj) T(std::forward<Args>(args)...);
        obj->mark_in_use();
        ++live_count_;
        return obj;
    }

    // å›æ”¶å¯¹è±¡ / Deallocate object
    void deallocate(T* p) noexcept {
        
        if (!p) return;
        p->~T();
        free_stack_.push(p);  // ç›´æ¥å‹å…¥ stack
        --live_count_;

    }

    // =============================================================
    // ğŸ”’ çº¿ç¨‹å®‰å…¨ APIï¼ˆæ± å†…éƒ¨åŒæ­¥ï¼‰
    // Thread-safe API (internal synchronization within the pool)
    // =============================================================
    template <class... Args>
    T* atomic_allocate(Args&&... args) {
        LockGuard g(lock_);
        return allocate(std::forward<Args>(args)...);
    }

    void atomic_deallocate(T* p) noexcept {
        if (!p) return;
        LockGuard g(lock_);
        deallocate(p);
    }

    void atomic_clear() noexcept {
        LockGuard g(lock_);
        clear();
    }

    // æ¸…ç©ºæ± å­ / Clear all memory
    void clear() noexcept {
        for (auto& seg : segments_) {
            ::operator delete[](seg.data, std::align_val_t(alignof(T)));
            seg.data = nullptr;
        }
        segments_.clear();
        live_count_ = 0;
        next_pages_hint_ = pages_per_segment_base_;
    }

    std::size_t live() const noexcept { return live_count_; }
    std::size_t segments() const noexcept { return segments_.size(); }
    std::size_t capacity_total() const noexcept {
        std::size_t c = 0; for (auto const& s : segments_) c += s.capacity; return c; }

private:
    
    // åˆ†é…å’Œå›æ”¶æ“ä½œçš„å…·ä½“å®ç°
    // The specific implementation of allocation and recycling operations

    std::size_t compute_min_pages(std::size_t user_min_pages) const noexcept {
        const std::size_t ps = page_size_;
        const std::size_t ss = slot_size_;
        const std::size_t l = detail::lcm(ps, ss);
        std::size_t min_pages = l / ps;
        if (user_min_pages > 0) {
            std::size_t k = (user_min_pages + min_pages - 1) / min_pages;
            min_pages *= k;
        }
        return min_pages;
    }

    void add_segment_() {
        if (segments_.empty()) {
            next_pages_hint_ = pages_per_segment_base_;
        } else {
            double target = static_cast<double>(next_pages_hint_) * growth_factor_;
            std::size_t pages = static_cast<std::size_t>(target);
            if (pages < next_pages_hint_ + pages_per_segment_base_)
                pages = next_pages_hint_ + pages_per_segment_base_;
            std::size_t rem = pages % pages_per_segment_base_;
            if (rem) pages += (pages_per_segment_base_ - rem);
            next_pages_hint_ = pages;
        }
        const std::size_t seg_bytes = next_pages_hint_ * page_size_;
        const std::size_t capacity = seg_bytes / slot_size_;
        std::byte* raw = reinterpret_cast<std::byte*>(::operator new[](seg_bytes, std::align_val_t(alignof(T))));
        segments_.emplace_back(raw, capacity);
    }

private:
    std::vector<Segment> segments_;
    std::size_t page_size_ = detail::os_page_size();
    std::size_t slot_size_ = 0;
    std::size_t pages_per_segment_base_ = 0;
    double growth_factor_ = 1.0;
    std::size_t next_pages_hint_ = 0;
    std::size_t live_count_ = 0;

    // Thread-safe lock
    SpinLock lock_;
};

// ----------------------------
// PooledObject åŸºç±» / Base class for pooled objects
// ----------------------------
template <class Derived>
struct PooledObject {
    virtual ~PooledObject() = default;
    virtual void reset() {}

    // ç”¨äºæè‡´æ€§èƒ½åœºæ™¯çš„çº¿ç¨‹ä¸å®‰å…¨åˆ›å»ºæ–¹æ³• / Thread-unsafe creation method for extreme performance scenarios
    static Derived* create(auto&&... args) {
        return SegmentedObjectPool<Derived>::instance().allocate(std::forward<decltype(args)>(args)...);
    }

    // çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬çš„åˆ›å»ºæ–¹æ³• Thread-safe version of the create method
    static Derived* atomic_create(auto&&... args) {
        return SegmentedObjectPool<Derived>::instance().atomic_allocate(std::forward<decltype(args)>(args)...);
    }

    // ç”¨äºæè‡´æ€§èƒ½åœºæ™¯çš„çº¿ç¨‹ä¸å®‰å…¨å›æ”¶æ–¹æ³• / Thread-unsafe recycle method for extreme performance scenarios
    inline void recycle() {
        this->reset();
        recycled_ = true;
        SegmentedObjectPool<Derived>::instance().deallocate(static_cast<Derived*>(this));
    }

    // çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬çš„å›æ”¶æ–¹æ³• Thread-safe version of the recycling method
    inline void atomic_recycle() {
        this->reset();
        recycled_ = true;
        SegmentedObjectPool<Derived>::instance().atomic_deallocate(static_cast<Derived*>(this));
    }

    inline bool is_recycled() const noexcept { return recycled_; }
    inline void mark_in_use() noexcept { recycled_ = false; }

private:
    bool recycled_ = false;
};
